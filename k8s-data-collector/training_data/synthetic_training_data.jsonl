{"id":"synthetic-0","resource_type":"synthetic","input":"Pod stuck in Pending state. Reason: PVC pending. Resource requests: CPU=85, Memory=256Mi","output":"Pod cannot be scheduled. Common causes: 1) Insufficient cluster resources, 2) Node selector/affinity conflicts, 3) PVC binding issues.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["scheduling","pending"]},"timestamp":"2025-11-10T01:24:29.134111086Z"}
{"id":"synthetic-1","resource_type":"synthetic","input":"Node CPU usage at 95%. Multiple pods throttled. Cluster autoscaling: disabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.134132385Z"}
{"id":"synthetic-2","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 1Gi. Last exit code: 137.","output":"Container exceeded memory limit of 1Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134147734Z"}
{"id":"synthetic-3","resource_type":"synthetic","input":"Node experiencing disk pressure. Available disk: 3GB. Condition: DiskPressure=True","output":"Node running low on disk space. Immediate actions: 1) Clean up old container images, 2) Review persistent volume claims, 3) Implement log rotation.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Critical","tags":["disk","storage"]},"timestamp":"2025-11-10T01:24:29.134162912Z"}
{"id":"synthetic-4","resource_type":"synthetic","input":"Node CPU usage at 98%. Multiple pods throttled. Cluster autoscaling: disabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.134182549Z"}
{"id":"synthetic-5","resource_type":"synthetic","input":"Service database-proxy returning 503 errors. Backend pods: 1/3 healthy. Load balancer status: degraded","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.134191305Z"}
{"id":"synthetic-6","resource_type":"synthetic","input":"Node CPU usage at 98%. Multiple pods throttled. Cluster autoscaling: enabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.134206033Z"}
{"id":"synthetic-7","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 4Gi. Last exit code: 137.","output":"Container exceeded memory limit of 4Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134220870Z"}
{"id":"synthetic-8","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: HTTP GET. Timeout: 10s. Failures: 3","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.134236209Z"}
{"id":"synthetic-9","resource_type":"synthetic","input":"Node CPU usage at 85%. Multiple pods throttled. Cluster autoscaling: disabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.134250786Z"}
{"id":"synthetic-10","resource_type":"synthetic","input":"Node CPU usage at 95%. Multiple pods throttled. Cluster autoscaling: enabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.134264973Z"}
{"id":"synthetic-11","resource_type":"synthetic","input":"Node experiencing disk pressure. Available disk: 1GB. Condition: DiskPressure=True","output":"Node running low on disk space. Immediate actions: 1) Clean up old container images, 2) Review persistent volume claims, 3) Implement log rotation.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["disk","storage"]},"timestamp":"2025-11-10T01:24:29.134271765Z"}
{"id":"synthetic-12","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 512Mi. Last exit code: 137.","output":"Container exceeded memory limit of 512Mi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134287074Z"}
{"id":"synthetic-13","resource_type":"synthetic","input":"Pod stuck in Pending state. Reason: Insufficient memory. Resource requests: CPU=95, Memory=1Gi","output":"Pod cannot be scheduled. Common causes: 1) Insufficient cluster resources, 2) Node selector/affinity conflicts, 3) PVC binding issues.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["scheduling","pending"]},"timestamp":"2025-11-10T01:24:29.134302332Z"}
{"id":"synthetic-14","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 4Gi. Last exit code: 137.","output":"Container exceeded memory limit of 4Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134316810Z"}
{"id":"synthetic-15","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: gcr.io/project/service:sha256. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.134332078Z"}
{"id":"synthetic-16","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: TCP. Timeout: 5s. Failures: 5","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.134346976Z"}
{"id":"synthetic-17","resource_type":"synthetic","input":"Service database-proxy returning 503 errors. Backend pods: 0/3 healthy. Load balancer status: healthy","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.134355161Z"}
{"id":"synthetic-18","resource_type":"synthetic","input":"Pod stuck in Pending state. Reason: PVC pending. Resource requests: CPU=98, Memory=256Mi","output":"Pod cannot be scheduled. Common causes: 1) Insufficient cluster resources, 2) Node selector/affinity conflicts, 3) PVC binding issues.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["scheduling","pending"]},"timestamp":"2025-11-10T01:24:29.134369999Z"}
{"id":"synthetic-19","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: docker.io/myapp:latest. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.134384736Z"}
{"id":"synthetic-20","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: HTTP GET. Timeout: 10s. Failures: 5","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.134399263Z"}
{"id":"synthetic-21","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: docker.io/myapp:latest. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.134413390Z"}
{"id":"synthetic-22","resource_type":"synthetic","input":"Pod in CrashLoopBackOff state. Restart count: 20. Last error: database connection failed","output":"Application failing to start. Troubleshooting: 1) Check logs with kubectl logs, 2) Verify configuration and environment variables, 3) Check dependencies.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Critical","tags":["crashloop","startup"]},"timestamp":"2025-11-10T01:24:29.134428849Z"}
{"id":"synthetic-23","resource_type":"synthetic","input":"Pod in CrashLoopBackOff state. Restart count: 5. Last error: database connection failed","output":"Application failing to start. Troubleshooting: 1) Check logs with kubectl logs, 2) Verify configuration and environment variables, 3) Check dependencies.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["crashloop","startup"]},"timestamp":"2025-11-10T01:24:29.134443306Z"}
{"id":"synthetic-24","resource_type":"synthetic","input":"Node CPU usage at 95%. Multiple pods throttled. Cluster autoscaling: enabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.134458163Z"}
{"id":"synthetic-25","resource_type":"synthetic","input":"Pod stuck in Pending state. Reason: No nodes available. Resource requests: CPU=98, Memory=512Mi","output":"Pod cannot be scheduled. Common causes: 1) Insufficient cluster resources, 2) Node selector/affinity conflicts, 3) PVC binding issues.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["scheduling","pending"]},"timestamp":"2025-11-10T01:24:29.134464886Z"}
{"id":"synthetic-26","resource_type":"synthetic","input":"Service web-frontend returning 503 errors. Backend pods: 1/3 healthy. Load balancer status: healthy","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.134480275Z"}
{"id":"synthetic-27","resource_type":"synthetic","input":"Pod in CrashLoopBackOff state. Restart count: 50. Last error: database connection failed","output":"Application failing to start. Troubleshooting: 1) Check logs with kubectl logs, 2) Verify configuration and environment variables, 3) Check dependencies.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Critical","tags":["crashloop","startup"]},"timestamp":"2025-11-10T01:24:29.134494591Z"}
{"id":"synthetic-28","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 4Gi. Last exit code: 137.","output":"Container exceeded memory limit of 4Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134508968Z"}
{"id":"synthetic-29","resource_type":"synthetic","input":"Node CPU usage at 95%. Multiple pods throttled. Cluster autoscaling: disabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.134523565Z"}
{"id":"synthetic-30","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: docker.io/myapp:latest. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.134537812Z"}
{"id":"synthetic-31","resource_type":"synthetic","input":"Node CPU usage at 95%. Multiple pods throttled. Cluster autoscaling: enabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.134554012Z"}
{"id":"synthetic-32","resource_type":"synthetic","input":"Node CPU usage at 98%. Multiple pods throttled. Cluster autoscaling: disabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.134560424Z"}
{"id":"synthetic-33","resource_type":"synthetic","input":"Node CPU usage at 98%. Multiple pods throttled. Cluster autoscaling: enabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.134575993Z"}
{"id":"synthetic-34","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: registry.example.com/app:v1.2.3. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.134590651Z"}
{"id":"synthetic-35","resource_type":"synthetic","input":"Pod stuck in Pending state. Reason: Insufficient memory. Resource requests: CPU=95, Memory=1Gi","output":"Pod cannot be scheduled. Common causes: 1) Insufficient cluster resources, 2) Node selector/affinity conflicts, 3) PVC binding issues.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["scheduling","pending"]},"timestamp":"2025-11-10T01:24:29.134605358Z"}
{"id":"synthetic-36","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: docker.io/myapp:latest. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.134619534Z"}
{"id":"synthetic-37","resource_type":"synthetic","input":"Pod stuck in Pending state. Reason: Insufficient cpu. Resource requests: CPU=90, Memory=4Gi","output":"Pod cannot be scheduled. Common causes: 1) Insufficient cluster resources, 2) Node selector/affinity conflicts, 3) PVC binding issues.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["scheduling","pending"]},"timestamp":"2025-11-10T01:24:29.134633961Z"}
{"id":"synthetic-38","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 2Gi. Last exit code: 137.","output":"Container exceeded memory limit of 2Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134640614Z"}
{"id":"synthetic-39","resource_type":"synthetic","input":"Pod in CrashLoopBackOff state. Restart count: 5. Last error: config file not found","output":"Application failing to start. Troubleshooting: 1) Check logs with kubectl logs, 2) Verify configuration and environment variables, 3) Check dependencies.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Critical","tags":["crashloop","startup"]},"timestamp":"2025-11-10T01:24:29.134654810Z"}
{"id":"synthetic-40","resource_type":"synthetic","input":"Service database-proxy returning 503 errors. Backend pods: 1/3 healthy. Load balancer status: unhealthy","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.134670039Z"}
{"id":"synthetic-41","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: Exec. Timeout: 10s. Failures: 5","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.134685247Z"}
{"id":"synthetic-42","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: Exec. Timeout: 3s. Failures: 10","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.134699744Z"}
{"id":"synthetic-43","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 2Gi. Last exit code: 137.","output":"Container exceeded memory limit of 2Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134714121Z"}
{"id":"synthetic-44","resource_type":"synthetic","input":"Service api-service returning 503 errors. Backend pods: 0/5 healthy. Load balancer status: unhealthy","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.134721284Z"}
{"id":"synthetic-45","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: TCP. Timeout: 3s. Failures: 3","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.134735411Z"}
{"id":"synthetic-46","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 4Gi. Last exit code: 137.","output":"Container exceeded memory limit of 4Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134749417Z"}
{"id":"synthetic-47","resource_type":"synthetic","input":"Service web-frontend returning 503 errors. Backend pods: 1/10 healthy. Load balancer status: unhealthy","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.134764806Z"}
{"id":"synthetic-48","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 4Gi. Last exit code: 137.","output":"Container exceeded memory limit of 4Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134778561Z"}
{"id":"synthetic-49","resource_type":"synthetic","input":"Node experiencing disk pressure. Available disk: 5GB. Condition: DiskPressure=True","output":"Node running low on disk space. Immediate actions: 1) Clean up old container images, 2) Review persistent volume claims, 3) Implement log rotation.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Critical","tags":["disk","storage"]},"timestamp":"2025-11-10T01:24:29.134793138Z"}
{"id":"synthetic-50","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: gcr.io/project/service:sha256. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.134799721Z"}
{"id":"synthetic-51","resource_type":"synthetic","input":"Node experiencing disk pressure. Available disk: 3GB. Condition: DiskPressure=True","output":"Node running low on disk space. Immediate actions: 1) Clean up old container images, 2) Review persistent volume claims, 3) Implement log rotation.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Critical","tags":["disk","storage"]},"timestamp":"2025-11-10T01:24:29.134814078Z"}
{"id":"synthetic-52","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 1Gi. Last exit code: 137.","output":"Container exceeded memory limit of 1Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134827893Z"}
{"id":"synthetic-53","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: Exec. Timeout: 10s. Failures: 3","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.134842140Z"}
{"id":"synthetic-54","resource_type":"synthetic","input":"Pod in CrashLoopBackOff state. Restart count: 10. Last error: config file not found","output":"Application failing to start. Troubleshooting: 1) Check logs with kubectl logs, 2) Verify configuration and environment variables, 3) Check dependencies.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Critical","tags":["crashloop","startup"]},"timestamp":"2025-11-10T01:24:29.134856847Z"}
{"id":"synthetic-55","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: registry.example.com/app:v1.2.3. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.134871284Z"}
{"id":"synthetic-56","resource_type":"synthetic","input":"Node CPU usage at 85%. Multiple pods throttled. Cluster autoscaling: enabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.134877786Z"}
{"id":"synthetic-57","resource_type":"synthetic","input":"Pod stuck in Pending state. Reason: No nodes available. Resource requests: CPU=85, Memory=4Gi","output":"Pod cannot be scheduled. Common causes: 1) Insufficient cluster resources, 2) Node selector/affinity conflicts, 3) PVC binding issues.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["scheduling","pending"]},"timestamp":"2025-11-10T01:24:29.134892434Z"}
{"id":"synthetic-58","resource_type":"synthetic","input":"Pod stuck in Pending state. Reason: No nodes available. Resource requests: CPU=95, Memory=256Mi","output":"Pod cannot be scheduled. Common causes: 1) Insufficient cluster resources, 2) Node selector/affinity conflicts, 3) PVC binding issues.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["scheduling","pending"]},"timestamp":"2025-11-10T01:24:29.134906600Z"}
{"id":"synthetic-59","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: Exec. Timeout: 5s. Failures: 3","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.134920817Z"}
{"id":"synthetic-60","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 1Gi. Last exit code: 137.","output":"Container exceeded memory limit of 1Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134934783Z"}
{"id":"synthetic-61","resource_type":"synthetic","input":"Node experiencing disk pressure. Available disk: 3GB. Condition: DiskPressure=True","output":"Node running low on disk space. Immediate actions: 1) Clean up old container images, 2) Review persistent volume claims, 3) Implement log rotation.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Critical","tags":["disk","storage"]},"timestamp":"2025-11-10T01:24:29.134949410Z"}
{"id":"synthetic-62","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: TCP. Timeout: 3s. Failures: 3","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.134955401Z"}
{"id":"synthetic-63","resource_type":"synthetic","input":"Pod in CrashLoopBackOff state. Restart count: 5. Last error: port already in use","output":"Application failing to start. Troubleshooting: 1) Check logs with kubectl logs, 2) Verify configuration and environment variables, 3) Check dependencies.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Critical","tags":["crashloop","startup"]},"timestamp":"2025-11-10T01:24:29.134970029Z"}
{"id":"synthetic-64","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 256Mi. Last exit code: 137.","output":"Container exceeded memory limit of 256Mi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.134984906Z"}
{"id":"synthetic-65","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 2Gi. Last exit code: 137.","output":"Container exceeded memory limit of 2Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.135001497Z"}
{"id":"synthetic-66","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: gcr.io/project/service:sha256. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.135007989Z"}
{"id":"synthetic-67","resource_type":"synthetic","input":"Service api-service returning 503 errors. Backend pods: 1/5 healthy. Load balancer status: healthy","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.135023098Z"}
{"id":"synthetic-68","resource_type":"synthetic","input":"Service database-proxy returning 503 errors. Backend pods: 1/5 healthy. Load balancer status: unhealthy","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.135038196Z"}
{"id":"synthetic-69","resource_type":"synthetic","input":"Pod stuck in Pending state. Reason: Insufficient cpu. Resource requests: CPU=90, Memory=2Gi","output":"Pod cannot be scheduled. Common causes: 1) Insufficient cluster resources, 2) Node selector/affinity conflicts, 3) PVC binding issues.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["scheduling","pending"]},"timestamp":"2025-11-10T01:24:29.135052663Z"}
{"id":"synthetic-70","resource_type":"synthetic","input":"Service web-frontend returning 503 errors. Backend pods: 1/3 healthy. Load balancer status: degraded","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.135067290Z"}
{"id":"synthetic-71","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: docker.io/myapp:latest. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.135081717Z"}
{"id":"synthetic-72","resource_type":"synthetic","input":"Pod in CrashLoopBackOff state. Restart count: 5. Last error: port already in use","output":"Application failing to start. Troubleshooting: 1) Check logs with kubectl logs, 2) Verify configuration and environment variables, 3) Check dependencies.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Critical","tags":["crashloop","startup"]},"timestamp":"2025-11-10T01:24:29.135095974Z"}
{"id":"synthetic-73","resource_type":"synthetic","input":"Pod in CrashLoopBackOff state. Restart count: 10. Last error: connection refused","output":"Application failing to start. Troubleshooting: 1) Check logs with kubectl logs, 2) Verify configuration and environment variables, 3) Check dependencies.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Critical","tags":["crashloop","startup"]},"timestamp":"2025-11-10T01:24:29.135102245Z"}
{"id":"synthetic-74","resource_type":"synthetic","input":"Node CPU usage at 90%. Multiple pods throttled. Cluster autoscaling: enabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.135116923Z"}
{"id":"synthetic-75","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 4Gi. Last exit code: 137.","output":"Container exceeded memory limit of 4Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.135131109Z"}
{"id":"synthetic-76","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: docker.io/myapp:latest. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.135145226Z"}
{"id":"synthetic-77","resource_type":"synthetic","input":"Service web-frontend returning 503 errors. Backend pods: 1/3 healthy. Load balancer status: degraded","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.135159863Z"}
{"id":"synthetic-78","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 512Mi. Last exit code: 137.","output":"Container exceeded memory limit of 512Mi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.135166505Z"}
{"id":"synthetic-79","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 4Gi. Last exit code: 137.","output":"Container exceeded memory limit of 4Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.135182595Z"}
{"id":"synthetic-80","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: docker.io/myapp:latest. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.135196822Z"}
{"id":"synthetic-81","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: registry.example.com/app:v1.2.3. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.135210888Z"}
{"id":"synthetic-82","resource_type":"synthetic","input":"Service database-proxy returning 503 errors. Backend pods: 1/3 healthy. Load balancer status: degraded","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.135225986Z"}
{"id":"synthetic-83","resource_type":"synthetic","input":"Pod stuck in Pending state. Reason: Insufficient memory. Resource requests: CPU=95, Memory=4Gi","output":"Pod cannot be scheduled. Common causes: 1) Insufficient cluster resources, 2) Node selector/affinity conflicts, 3) PVC binding issues.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["scheduling","pending"]},"timestamp":"2025-11-10T01:24:29.135240443Z"}
{"id":"synthetic-84","resource_type":"synthetic","input":"Node experiencing disk pressure. Available disk: 5GB. Condition: DiskPressure=True","output":"Node running low on disk space. Immediate actions: 1) Clean up old container images, 2) Review persistent volume claims, 3) Implement log rotation.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["disk","storage"]},"timestamp":"2025-11-10T01:24:29.135254660Z"}
{"id":"synthetic-85","resource_type":"synthetic","input":"Service database-proxy returning 503 errors. Backend pods: 0/10 healthy. Load balancer status: healthy","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.135269397Z"}
{"id":"synthetic-86","resource_type":"synthetic","input":"Node CPU usage at 90%. Multiple pods throttled. Cluster autoscaling: enabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.135284235Z"}
{"id":"synthetic-87","resource_type":"synthetic","input":"Service web-frontend returning 503 errors. Backend pods: 0/5 healthy. Load balancer status: degraded","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.135298692Z"}
{"id":"synthetic-88","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: TCP. Timeout: 10s. Failures: 3","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.135313029Z"}
{"id":"synthetic-89","resource_type":"synthetic","input":"Node experiencing disk pressure. Available disk: 0.5GB. Condition: DiskPressure=True","output":"Node running low on disk space. Immediate actions: 1) Clean up old container images, 2) Review persistent volume claims, 3) Implement log rotation.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Critical","tags":["disk","storage"]},"timestamp":"2025-11-10T01:24:29.135319260Z"}
{"id":"synthetic-90","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: HTTP GET. Timeout: 5s. Failures: 3","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.135333737Z"}
{"id":"synthetic-91","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: HTTP GET. Timeout: 3s. Failures: 10","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.135347974Z"}
{"id":"synthetic-92","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: Exec. Timeout: 10s. Failures: 3","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.135361830Z"}
{"id":"synthetic-93","resource_type":"synthetic","input":"Pod experiencing OOMKilled errors. Container memory limit: 1Gi. Last exit code: 137.","output":"Container exceeded memory limit of 1Gi. Solution: 1) Increase memory limits, 2) Profile application memory usage, 3) Check for memory leaks.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Critical","tags":["oom","memory"]},"timestamp":"2025-11-10T01:24:29.135376046Z"}
{"id":"synthetic-94","resource_type":"synthetic","input":"Service web-frontend returning 503 errors. Backend pods: 1/3 healthy. Load balancer status: healthy","output":"Service degraded. Troubleshooting: 1) Check pod readiness probes, 2) Verify service selector matches pods, 3) Review endpoint configuration.","metadata":{"namespace":"prod-4","cluster":"training-cluster","severity":"Critical","tags":["service","networking"]},"timestamp":"2025-11-10T01:24:29.135390573Z"}
{"id":"synthetic-95","resource_type":"synthetic","input":"Pod stuck in Pending state. Reason: No nodes available. Resource requests: CPU=95, Memory=1Gi","output":"Pod cannot be scheduled. Common causes: 1) Insufficient cluster resources, 2) Node selector/affinity conflicts, 3) PVC binding issues.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["scheduling","pending"]},"timestamp":"2025-11-10T01:24:29.135405171Z"}
{"id":"synthetic-96","resource_type":"synthetic","input":"Node CPU usage at 85%. Multiple pods throttled. Cluster autoscaling: enabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-1","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.135411793Z"}
{"id":"synthetic-97","resource_type":"synthetic","input":"Pod stuck in ImagePullBackOff. Image: docker.io/myapp:latest. Error: Failed to pull image.","output":"Cannot pull container image. Solutions: 1) Verify image exists in registry, 2) Check image pull secrets, 3) Verify network connectivity to registry.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["image","registry"]},"timestamp":"2025-11-10T01:24:29.135426160Z"}
{"id":"synthetic-98","resource_type":"synthetic","input":"Pod failing liveness probe. Probe: HTTP GET. Timeout: 5s. Failures: 10","output":"Health check failing. Investigation: 1) Verify endpoint responds within timeout, 2) Check application health, 3) Review probe configuration.","metadata":{"namespace":"prod-2","cluster":"training-cluster","severity":"Warning","tags":["probe","health"]},"timestamp":"2025-11-10T01:24:29.135440617Z"}
{"id":"synthetic-99","resource_type":"synthetic","input":"Node CPU usage at 90%. Multiple pods throttled. Cluster autoscaling: enabled","output":"High CPU utilization detected. Actions: 1) Review pod resource requests/limits, 2) Consider horizontal pod autoscaling, 3) Add nodes if autoscaling disabled.","metadata":{"namespace":"prod-3","cluster":"training-cluster","severity":"Warning","tags":["cpu","resources"]},"timestamp":"2025-11-10T01:24:29.135455084Z"}
